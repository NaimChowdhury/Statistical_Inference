---
title: 'Statistics Assignment #7'
author: "Naeem Chowdhury"
date: "6/16/2020"
output: pdf_document
---


##1. Setup
### options
Set up global options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
knitr::opts_chunk$set(fig.height=4, fig.width=6)
```

### libraries
Load in needed libraries 
```{r, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
library(haven)
```

## 2. File management
### Create variables for directories
```{r file_management, include=FALSE}
project.dir <- getwd() #naeem
output.dir <- "/Output"
data.dir <- "C:/Users/Naeem Cho/Desktop/School Work/Statistical Inference/Datasets"
setwd(project.dir)
getwd()
```

## 3. Importing Data
```{r, echo = FALSE, include=FALSE}
fl_crime <- read_csv(file.path(data.dir, "fl_crime.csv"))

fl_crime <- fl_crime %>% 
  rename(crime = `crime rate (per 1000)`) %>%
  rename(education = `education (%)`) %>% 
  rename(urbanization = `urbanization (%)`)

```

# Problem #1

For the _FL_crime.csv_ data, proceed to fit

  1. For simple linear regression _crime ~ education_,
    
    a. Write down the __full modeling equation__, with all __error assumptions__.
    b. Fit the model, provide the __fitted equation__. Provide a plot of the fitted line. Is there a statistically significant relationship? Interpret the resulting effect of education on crime.

### 1a.

$$crime = \beta_0 + \beta_1 \cdot education + \epsilon, \ \ \epsilon \sim (N, \sigma^2). $$

### 1b.

```{r}
lm.obj <- lm(crime ~ education, fl_crime)
summary(lm.obj)
```

The fitted equation is thus,

$$\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_{1,i},$$

where $\hat{y_i}$ _crime_, $x_1$ is _education_, $\hat{\beta_0}$ is -50.86, and $\hat{\beta_1}$ is 1.49.

$$crime =  -50.86 + 1.49 \cdot education$$
   
```{r}
ggplot(fl_crime, aes(x = education, y = crime)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```

There is indeed a statistically significant relationship, since we have $p \approx 0.000068 < 0.025$ for the slope of the linear regression. For every 2\% increase in education, we can conclude that the number of crimes per 1000 will increase by about 3, on average.

  2. For multiple linear regression _crime ~ education + urbanization_,
    a. Write down the __full modeling equation__, with all __error assumptions__.
    b. Fit the model, provide the __fitted equation__. Provide a plot of the __fitted plane__. Describe the relationship between crime and education now. Why did it change comparied to part 1? What statistical phenomena did we encounter in part 1 that led to such non-sensical interpretation?


### 2a.  

$$crime = \beta_0 + \beta_1 \cdot education + \beta_2 \cdot urbanization + \epsilon, \ \ \epsilon \sim (N, \sigma^2). $$


### 2b. 
```{r}
lm.obj <- lm(crime ~ education + urbanization, fl_crime)

summary(lm.obj)
```

The fitted equation is thus,

$$\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_{1,i} + \hat{\beta_2} x_{2,i},$$

where $\hat{y_i}$ _crime_, $x_1$ is _education_, $x_2$ is _urbanization_, $\hat{\beta_0}$ is 59.12, $\hat{\beta_1}$ is -0.58, and $\hat{\beta_2}$ is 0.68.

$$crime =  59.12 -0.58 \cdot education + 0.68 \cdot urbanization$$

The resulting $p$-value from hypothesis testing is sufficiently low to conclude that $\beta_2$ coefficient for _urbanization_ is statistically significant. However, the _education_ coefficient $\beta_1$ now has $p \approx 0.22 > 0.025$, meaning we fail to reject the null hypothesis $H_0:= \beta_1 = 0$. 

The change in relationship between _crime_ and _education_ seems to have occured because _education_ may be confounding with another variable, such as _urbanization_.

We can choose _education_ to be some arbitrary constant, suppose we choose $education = 65$. Then our equation becomes:

$$crime =  59.12 -0.58 \cdot 65 + 0.68 \cdot urbanization$$

$$=  59.12 -37.7 + 0.68 \cdot urbanization$$

$$=  21.42 + 0.68 \cdot urbanization,$$

which we can plot as a plane accordingly.

```{r}
ggplot(fl_crime, aes(x = urbanization, y = crime + 21.42)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

```

  3. For multiple linear gression _crime ~ education + urbanization + income_, proceed to
  
    a. Write down the __full modeling equation__, with all __error assumptions__.
    b. Show that $y_i \sim N(\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i}, \ \ \sigma^2) $ (where $y = crime$, $x_1 = education$ ...) First, show that $E[y_i]  = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i}$. Second, $V[y_i] = \sigma^2$. Third, $y_i \sim N$. No need to demonstrate the independence of $y_i$'s.
    c. Having fitted the model from $(a)$, provide the __fitted equation__.
    d. Write down the hypotheses ( in terms of parameters of the model in part (a)) and make conclusions for $t$-tests on significance of each __individual__ predictor.
    e. Interpret the effect of the only statistically significant predictor from part (d).
    f. Formulate the hypotheses (in terms of parameterse of the model in part (a)) for testing the overall model significance. Provide the conclusion of the respective test.
    
# Problem #2 (Why need $F$-statistic?)
    
  1. Generate a data example where you have a response variable $y$ and a predictor variable $x$ that are _unrelated_ to each other (make sure to use a __random__ generation mechanism). How would you do that? How would you demonstrate that they're unrelated (think of basic visualizations)?


### Solution 1.
```{r}
y <- runif(1000, min = 0, max = 1000) 
x <- runif(1000, min = 0, max = 3000) 

rand.df <- data.frame(x,y)

ggplot(rand.df, aes(x=x, y=y))+
  geom_point()+
  stat_smooth(method = 'lm')
```

In order to make sure the response and predictors were _unrelated_, I genderated them both by sampling from a random uniform distribution. To demonstrate they are unrelated, we can simply create a simple linear regression model and show that the slope of the model is practically 0, as shown.
  
  2. Having settled on a method of generating such unrelated variables in part 1, proceed to:
  
    a. Generate response variable $y$ (e.g. of length 200)
    b. Generate 50 predictor variables $x$ according to your method from part 1. __Record them__.

### 2a. and 2b.

```{r}
y <- y <- runif(200, min = 0, max = 1000) 

df <- data.frame(y)

set.seed(2)
# Make 50 columns of random uniformly distributed values
for(i in 1:50){
  x <- runif(200, min = -1000, max = 1000)
  df <- cbind(df, x)
}

# Give the columns of the dataframe unique names.
var_names <- sprintf("X%s", 0:50)
var_names[1] <- "Y"
var_names

colnames(df) <- var_names
df
```


  3. Fit a __multiple__ linear regression model, regression response $y$ from part 2(a) on all 50 $x$'s you've generated in part 2(b).

    a. Report the \# of individual $t$-tests that resulted into a significant $p$-value, hence rejecting $H_0$ at $\alpha = 0.05$ level. Were those rejections correct decisions or not? Why? ( __Hint:__ Remember what's the true relationship between $x$ and $y$, given how you generated the data.) If not, what type of error do they correspond to? Why?
    b. Given that the individual siginifant $t$-test aren't necessarily indicative of at least one predictor having a true relationship with the response, what would be the appropriate testing procedure to address that question? Conduct that test, report its $p$-value, and interpret its result.
    
For reference, use in-class demo (slide #42).

### 3a.

```{r}
lm.obj <- lm(Y~., df)
# Take only the p value portion of the summary
p_values <- summary(lm.obj)[4]
# Find which column has the p values
# p_values[,4]

# List of variables which have a value that implies statistical significance
significant_variables <- p_values[,4][p_values[,4] < 0.025]
```
There are two $p$-values and associated terms which lead to the rejection of the null hypothesis under the $t$-test. Those variables are $y$-intercept $\beta_0$, and the coefficient $\beta_17$.

# Problem #3

This question involves the use of multiple linear regression on the _Auto_ data set of _ISLR_ library.

  1. Produce a scatterplot matrix which includes all of the variables in the data set. Which variables appear to have a strong linear relationship with our intended response variable - miles per gallon ( _mpg_ )?
  
  2. Compute the matrix of correlations between the variables using the function _cor()_, to confirm your observation from part 1. Which predictors have strongest linear relationship with _mpg_?
  
  3. Pick one predictor variable that you feel to have the strongest __linear__ relationship with _mpg_, and preform a simple linear regression. Use the _summary()_ fnction to print the results.
  
    a. Is there a statistically significant relationship between the predictor and the response? Provide the interpretation of that relationship.
    b. What is the predicted _mpg_ associated with the median value of you predictor's range? Interpret that prediction. What are the associated 95% confidence and prediction bands? Provide interpreation of those intervals.
    c. Provide and interpet both metrics for the qualify of model fit.
    
4. Use the _lm()_ function to perform a multiple linear regression with _mpg_ as the response and all other variables (except _name_) as the predictors. Use the _summary()_ function to print the results.

  a. Formulate the $H_0$ and $H_a$ hypotheses (using parameter notation) for testing whether the overall model is significant. Which part of _summary()_ output corresponds to this test? Is the model significant?
  b. Which predictors appear to have a statistically significant relationship to the response? Just list them. 
  c. Interpret effects of the __two__ most statistically significant predictors. Compare the interpretation here, with the one given in part 3(a) - what's the crucial difference?
  d. Report and interpret the 95% confidence intervals for _weight_ and _year_ effects.
  e. Report and interpret both quality-of-fit metrics.

---
title: 'Statistics Assignment #7'
author: "Naeem Chowdhury"
date: "6/16/2020"
output: pdf_document
---


##1. Setup
### options
Set up global options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
knitr::opts_chunk$set(fig.height=4, fig.width=6)
```

### libraries
Load in needed libraries 
```{r, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
library(haven)
library(plotly)
#install.packages('rgl')
library(rgl)
library(ISLR)
```

## 2. File management
### Create variables for directories
```{r file_management, include=FALSE}
project.dir <- getwd() #naeem
output.dir <- "/Output"
data.dir <- "C:/Users/Naeem Cho/Desktop/School Work/Statistical Inference/Datasets"
setwd(project.dir)
getwd()
```

## 3. Importing Data
```{r, echo = FALSE, include=FALSE}
fl_crime <- read_csv(file.path(data.dir, "fl_crime.csv"))

fl_crime <- fl_crime %>% 
  rename(crime = `crime rate (per 1000)`) %>%
  rename(education = `education (%)`) %>% 
  rename(urbanization = `urbanization (%)`)

```

# Problem #1

For the _FL_crime.csv_ data, proceed to fit

  1. For simple linear regression _crime ~ education_,
    
    a. Write down the __full modeling equation__, with all __error assumptions__.
    b. Fit the model, provide the __fitted equation__. Provide a plot of the fitted line. Is there a statistically significant relationship? Interpret the resulting effect of education on crime.

### 1a.

$$crime = \beta_0 + \beta_1 \cdot education + \epsilon, \ \ \epsilon \sim_{iid} N(0, \sigma^2). $$

### 1b.

```{r}
lm.obj <- lm(crime ~ education, fl_crime)
summary(lm.obj)
```

The fitted equation is thus,

$$\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_{1,i},$$

where $\hat{y_i}$ _crime_, $x_1$ is _education_, $\hat{\beta_0}$ is -50.86, and $\hat{\beta_1}$ is 1.49.

$$crime_i =  -50.86 + 1.49 \cdot education_i$$
   
```{r}
ggplot(fl_crime, aes(x = education, y = crime)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")
```

There is indeed a statistically significant relationship, since for confidence level 90% we have $p \approx 0.000068 < 0.05$ for the slope of the linear regression. For every 2% increase in education, we can expect that the number of crimes per 1000 will increase by about 3, on average.

  2. For multiple linear regression _crime ~ education + urbanization_,
    a. Write down the __full modeling equation__, with all __error assumptions__.
    b. Fit the model, provide the __fitted equation__. Provide a plot of the __fitted plane__. Describe the relationship between crime and education now. Why did it change compared to part 1? What statistical phenomena did we encounter in part 1 that led to such non-sensical interpretation?


### 2a.  

$$crime = \beta_0 + \beta_1 \cdot education + \beta_2 \cdot urbanization + \epsilon, \ \ \epsilon \sim_{iid} N(0, \sigma^2). $$


### 2b. 
```{r}
lm.obj <- lm(crime ~ education + urbanization, fl_crime)

summary(lm.obj)
```

The fitted equation is thus,

$$\hat{y_i} = \hat{\beta_0} + \hat{\beta_1} x_{1,i} + \hat{\beta_2} x_{2,i},$$

where $\hat{y_i}$ _crime_, $x_1$ is _education_, $x_2$ is _urbanization_, $\hat{\beta_0}$ is 59.12, $\hat{\beta_1}$ is -0.58, and $\hat{\beta_2}$ is 0.68.

$$crime_i =  59.12 -0.58 \cdot education_i + 0.68 \cdot urbanization_i$$

The resulting $p$-value from hypothesis testing is sufficiently low to conclude that $\beta_2$ coefficient for _urbanization_ is statistically significant. However, the _education_ coefficient $\beta_1$ now has $p \approx 0.22 > 0.05$, meaning we fail to reject the null hypothesis $H_0:= \beta_1 = 0$. 

The change in relationship between _crime_ and _education_ seems to have occured increase _education_ may be correlated with increase in another variable, such as _urbanization_. 

We can plot as a plane accordingly.

```{r}
ggplot(fl_crime, aes(x = urbanization, y = crime + 21.42)) + 
  geom_point() +
  stat_smooth(method = "lm", col = "red")

plot3d(lm.obj, size = 5)

# segments3d(rep(TV, each=2),
#           rep(radio, each=2),
#           z=matrix(t(cbind(sales,predict(lm.obj))), nc=1),
#           add=T,
#           lwd=2,
#           col=2)
```

  3. For multiple linear gression _crime ~ education + urbanization + income_, proceed to
  
    a. Write down the __full modeling equation__, with all __error assumptions__.
    b. Show that $y_i \sim N(\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i}, \ \ \sigma^2) $ (where $y = crime$, $x_1 = education$ ...) First, show that $E[y_i]  = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i}$. Second, $V[y_i] = \sigma^2$. Third, $y_i \sim N$. No need to demonstrate the independence of $y_i$'s.
    c. Having fitted the model from $(a)$, provide the __fitted equation__.
    d. Write down the hypotheses ( in terms of parameters of the model in part (a)) and make conclusions for $t$-tests on significance of each __individual__ predictor.
    e. Interpret the effect of the only statistically significant predictor from part (d).
    f. Formulate the hypotheses (in terms of parameters of the model in part (a)) for testing the overall model significance. Provide the conclusion of the respective test.
    
### 3a.

We first show that $E[y_i]  = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i}$.

Consider,

$$y_i =  \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i} + \epsilon.$$

Then,

$$E[y_i] =  E[\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i} + \epsilon.].$$

By the linearity of expectation,

$$E[y_i] =  E[\beta_0] + E[\beta_1 x_{1,i}] + E[\beta_2 x_{2,i}] + E[\beta_3 x_{3,i}] + E[\epsilon].$$

Since $\beta_i$ are constants,

$$E[y_i] =  \beta_0 + \beta_1E[ x_{1,i}] + \beta_2 E[x_{2,i}] + \beta_3 E[x_{3,i}] + E[\epsilon].$$

And since $x_[i,j]$ are all fixed values,

$$E[y_i] =  \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i}+ E[\epsilon].$$

Finally, since $\epsilon \sim N(0, \sigma^2)$, $E[\epsilon] = 0$.

Thus,

$$E[y_i]  = \beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i},$$

as we wished to show.

Next, we show that 

$$V[y_i] = \sigma^2.$$

$$V[y_i] = V[\beta_0 + \beta_1 x_{1,i} + \beta_2 x_{2,i} + \beta_3 x_{3,i} + \epsilon].$$

We know by the linearity of variance that,

$$= V[\beta_0] + V[\beta_1 x_{1,i}] + V[\beta_2 x_{2,i}] + V[\beta_3 x_{3,i}] + V[\epsilon].$$

But $x_[i,j]$ and $\beta_i$ do not vary. So,

$$V[y_i] = V[\epsilon]$$.

Finally, since $\epsilon \sim N(0, \sigma^2)$, $V[\epsilon] = \sigma^2$.

Thus,

$$V[y_i] = \sigma^2,$$

as required.

    
# Problem #2 (Why need $F$-statistic?)
    
  1. Generate a data example where you have a response variable $y$ and a predictor variable $x$ that are _unrelated_ to each other (make sure to use a __random__ generation mechanism). How would you do that? How would you demonstrate that they're unrelated (think of basic visualizations)?


### Solution 1.
```{r}
y <- runif(1000, min = 0, max = 1000) 
x <- runif(1000, min = 0, max = 3000) 

rand.df <- data.frame(x,y)

ggplot(rand.df, aes(x=x, y=y))+
  geom_point()+
  stat_smooth(method = 'lm')
```

In order to make sure the response and predictors were _unrelated_, I genderated them both by sampling from a random uniform distribution. To demonstrate they are unrelated, we can simply create a simple linear regression model and show that the slope of the model is practically 0, as shown.
  
  2. Having settled on a method of generating such unrelated variables in part 1, proceed to:
  
    a. Generate response variable $y$ (e.g. of length 200)
    b. Generate 50 predictor variables $x$ according to your method from part 1. __Record them__.

### 2a. and 2b.

```{r}
y <- y <- runif(200, min = 0, max = 1000) 

df <- data.frame(y)


# Make 50 columns of random uniformly distributed values
for(i in 1:50){
  x <- runif(200, min = -1000, max = 1000)
  df <- cbind(df, x)
}

# Give the columns of the dataframe unique names.
var_names <- sprintf("X%s", 0:50)
var_names[1] <- "Y"
# var_names

colnames(df) <- var_names
# df
```


  3. Fit a __multiple__ linear regression model, regression response $y$ from part 2(a) on all 50 $x$'s you've generated in part 2(b).

    a. Report the \# of individual $t$-tests that resulted into a significant $p$-value, hence rejecting $H_0$ at $\alpha = 0.05$ level. Were those rejections correct decisions or not? Why? ( __Hint:__ Remember what's the true relationship between $x$ and $y$, given how you generated the data.) If not, what type of error do they correspond to? Why?
    b. Given that the individual siginifant $t$-test aren't necessarily indicative of at least one predictor having a true relationship with the response, what would be the appropriate testing procedure to address that question? Conduct that test, report its $p$-value, and interpret its result.
    
For reference, use in-class demo (slide #42).

### 3a.

```{r}
lm.obj <- lm(Y~., df)
# Take only the p value portion of the summary
p_values <- summary(lm.obj)$coefficients
# Find which column has the p values
# p_values[,4]

# List of variables which have a value that implies statistical significance
significant_variables <- p_values[,4][p_values[,4] < 0.05]
```
There are 3 $p$-values and associated terms which lead to the rejection of the null hypothesis under the $t$-test with 95% confidence level. Those variables are $y$-intercept $\beta_0$, and two coefficients $\beta_i$ and $\beta_j$.

This doesn't really make sense, since I generated all variables independent of one another, with uniform random distributions. 

However, upon closer inspection, we remember that this corresponds to a Type I error. We rejected 

$$H_0:= (\beta_i = 0, \ \forall i\in \mathbb{N}),$$ 

even though by construction $H_0$ is true. This gives concrete evidence to the interpretation of the $\alpha = 0.05$ value, which gives a 5% rate of Type I errors.

### 3b.

The appropriate testing procedure would be the $F$-test. With the $F$-test, we can assess whether at least one predictor has a strong relationship with the response variable. A low $F$-statistic would imply that the ratio of variance esxplained by our model to unexplained variance is close to 0, thus the perceived relationship we found in the previous $t$-test is due to unexplained variance.

```{r}
# F-test
summary(lm.obj)
```

With such an incredibly small value for the $F$-statistic, we should be able to say that none of the predictors have a strong relationship with the response variable. We fail to reject the null hypothesis for confidence level 90%,

$$H_0:= (\beta_0, \beta_1, ..., \beta_50) = 0$$

Since $p = 0.73 > 0.05$.

Thus we cannot be sure that all of the $\beta_i$'s are not 0.

# Problem #3

This question involves the use of multiple linear regression on the _Auto_ data set of _ISLR_ library.

  1. Produce a scatterplot matrix which includes all of the variables in the data set. Which variables appear to have a strong linear relationship with our intended response variable - miles per gallon ( _mpg_ )?
  
### 1.

```{r}
pairs(Auto, pch = 19, lower.panel = NULL)
```
Displacement, horsepower, and weight all seem to have strong linear relationships with _mpg_.

### Solution 1 end.
  
  2. Compute the matrix of correlations between the variables using the function _cor()_, to confirm your observation from part 1. Which predictors have strongest linear relationship with _mpg_?
  
### 2.

```{r}
auto.continuous <- Auto %>% 
  select(-year, -origin, -name)


cor(auto.continuous)
```


As claimed, _displacement_, _horsepower_, and _weight_ are all have the strongest linear relationship with _mpg_.

### Solution 2 end.

  3. Pick one predictor variable that you feel to have the strongest __linear__ relationship with _mpg_, and preform a simple linear regression. Use the _summary()_ fnction to print the results.
  
    a. Is there a statistically significant relationship between the predictor and the response? Provide the interpretation of that relationship.
    b. What is the predicted _mpg_ associated with the median value of you predictor's range? Interpret that prediction. What are the associated 95% confidence and prediction bands? Provide interpreation of those intervals.
    c. Provide and interpret both metrics for the qualify of model fit.
    
### 3a.

```{r}
lm.obj <- lm(mpg ~ weight, Auto)
summary(lm.obj)
```

Since $p < 0.5$ for the coefficient of _weight_, we reject the null hypothesis and conclude that the relationship between _weight_ and _mpg_ is statistically significant.

### 3b.

The equation of the fitted line is,

$$mpg = 46.2165 -0.007647 \cdot weight$$

We can first calculate the median,
```{r}
median <- median(Auto$weight)
median
```
and finally plug it into our fitted equation.

```{r}
46.2165 -0.007647*median
```

Thus, for the median value of _weight_, the predicted _mpg_ of a car is 24.78 MPG.

### 3c.

For our model, we have,

$$R^2 = 0.69,$$

so that we can say $69%$ of the variance is explained by the model. 

We also have that,

$$RSE = 4.33$$

and given that our predictor variable is _mpg_, this is actually fairly significant. 

The model is decent, but there is much that is left unexplained by the model, and it is can be seen in the RSE.

### Solution 3 end.

4. Use the _lm()_ function to perform a multiple linear regression with _mpg_ as the response and all other variables (except _name_) as the predictors. Use the _summary()_ function to print the results.

  a. Formulate the $H_0$ and $H_a$ hypotheses (using parameter notation) for testing whether the overall model is significant. Which part of _summary()_ output corresponds to this test? Is the model significant?
  b. Which predictors appear to have a statistically significant relationship to the response? Just list them. 
  c. Interpret effects of the __two__ most statistically significant predictors. Compare the interpretation here, with the one given in part 3(a) - what's the crucial difference?
  d. Report and interpret the 95% confidence intervals for _weight_ and _year_ effects.
  e. Report and interpret both quality-of-fit metrics.

### Solution 4.

```{r}
auto.continuous <- Auto %>% select(-name)

lm.obj <- lm(mpg~., auto.continuous)
summary(lm.obj)
```

### 4a. 

For this problem, we use the $F$-test to test whether the overall model is significant. Our hypotheses are:


$$H_0:= (\beta_i = 0, \ \ \forall i \in \mathbb{N}$$

$$H_a:= (\exists i \in \mathbb{N} \ \ st. \ \ \beta_i \neq 0)$$

The last line of the summary output, the $F$-statistic and its corresponding $p$ value, refers to the results of this test. Since $p < 0.05$ we can conclude that for a 90% confidence level, we reject the null hypothesis. That is, our model is significant, there is at least one coefficient which is non-zero.


### 4b.



### 4c.

### 4d.


### 4e.
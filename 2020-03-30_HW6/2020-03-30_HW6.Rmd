---
title: '2020-03-30 HW6'
author: "Naeem Chowdhury"
date: "3/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r file_management, include=FALSE}
project.dir <- getwd() #naeem
output.dir <- "/Output"
data.dir <- "C:/Users/Naeem Cho/Desktop/School Work/Statistical Inference/Datasets"
setwd(project.dir)
getwd()
```

```{r, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(haven)
```

```{r, echo = FALSE, include=FALSE}
advert.df <- read_csv(file.path(data.dir, "Advertising.csv"))
```
# Problem #1

Proceed to write your own function _my.simple.lm()_ which, as inputs, will take

    * vector $x$ of explanatory variable values, and
    * vector $y$ of corresponding response variable values,
    
and output the following:

    * least squares estimates for intercept and slope,
    * standard errors of those estimates,
    * residual standard error of the least squares fit,
    * $R^2$ value,
    * 95% confidence intervals for least squares estimates.
    
You will need to calculate those outputs by directly applying the formulas for those quantitites. In your function definition, you are __not allowed to use any of $R$'s "cheat" built-in functions__ like _lm()_, _glm()_, _summary()_ etc.

```{r}
my.simple.lm <- function(x,y){
  n <- length(x)
  x.bar <- mean(x) # compute explanatory mean
  y.bar <- mean(y) # compute response mean
  
  x.dif <- (x-x.bar)
  y.dif <- (y-y.bar)
  
  beta1 <- ( sum( x.dif*y.dif ) )/ sum( x.dif^2 ) # compute beta1
  beta0 <- y.bar - beta1*x.bar # compute beta1
  
  y.hat <- beta0 + beta1*x
  
  RSS <- sum( (y - y.hat)^2 )
  TSS <- sum( y.dif^2)
  RSE <- sqrt(RSS/(n-2)) # compute RSE
  sig2 <- RSS/(n-2)
  
  # compute standard error of beta0 (SE.beta0)
  SE.beta0 <- sqrt(sig2 * (1/n + x.bar^2/sum( x.dif^2) ) )
  # compute standard error of beta1 (SE.beta1)
  SE.beta1 <- sqrt(sig2 / sum(x.dif^2) )
  
  R2 <- 1 - RSS/TSS# compute R^2
  
  # compute 95% confidence intervals for least squares estimates.
  interval.beta0 <- c(beta0 - 2*SE.beta0, beta0 + 2*SE.beta0)
  interval.beta1 <- c(beta1 - 2*SE.beta1, beta1 + 2*SE.beta1)
  
  print( c("Beta_0 estimate: ", beta0) )
  print( c("Beta_1 estimate: ", beta1) )
  print( c("Beta_0 standard error: ", SE.beta0) )
  print( c("Beta_1 standard error: ", SE.beta1) )
  print( c("Residual error of least squares fit: ", RSE))
  print( c("R^2 value: ", R2))
}
```


# Problem #2

Presume data $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, we have fitted simple linear regression equation

$$\hat{y_i} = \hat{\beta_0} + \hat{\beta_1}x_i,  \ \ i = 1, ..., n$$

where $\hat{\beta_0}$ and $\hat{\beta_1}$ are least squares estimates. Proceed to derive that

(a) (did it in class) $\sum_i e_i = 0$, where $e_i = \hat{y}_i -y_i$.

(b) (2 bonus points) $\sum_i x_i e_i = 0$

## Part a
Show that $\sum_i e_i = 0$, where $e_i = \hat{y}_i - y_i$.

### Proof

  We wish to show that $e_i = \hat{y}_i - y_i = \hat{\beta}_0 + \hat{\beta}_1x_i - y_i= 0$.
  
  Recall that since $\hat{\beta}_0$ is the least squares estimate, it must be that $\hat{\beta}_0$ minimizes the RSE, i.e. that,
  
  $$\frac{\partial}{\partial \hat{\beta}_0} \sum_i^n ( \hat{\beta}_0 + \hat{\beta}_1x_i - y_i)^2$$
  
  $$ = -2\sum_i^n ( \hat{\beta}_0 + \hat{\beta}_1x_i - y_i)$$
  
  $$ = 0.$$
  
  Finally, it is clear that
  
  $$-2 \sum_i^n ( \hat{\beta}_0 + \hat{\beta}_1x_i - y_i)$$
  
  $$ = -2 \sum_i^n (e_i) = 0,$$
  
  necessarily implies that $e_i = 0$, as required.

## Part 1b

Show that $\sum_i x_i e_i = 0$.

### Proof

  Similarly, we note that in order for $\hat{\beta}_1$ to minimize the residual sum of squares, we must have that,
  
  $$\frac{\partial}{\partial \hat{\beta}_1} \sum_i^n ( \hat{\beta}_0 + \hat{\beta}_1x_i - y_i)^2 = 0.$$
  
  Taking the partial derivative with respect to $\hat{\beta}_1$, we arrive at 
  
  $$-2 \sum_i^n ( \hat{\beta}_0 + \hat{\beta}_1x_i - y_i)x_i$$
  
  $$ = -2 \sum_i^n x_ie_i = 0.$$
  
  Giving the required result.
  
## Problem 2.

Presume we have $n = 3$ observations:

  - vector of predictor values $x = (x_1, x_2, x_3)$ = c(1,5,3),
  - a vector of residuals $\vec{e} = (e_1, e_2, e_3)$, and you let $e_1 = 3$.
  
Proceed to use the results from part 1(a,b) to derive the values of $e_2, e_3$. Given that you were able to do that, what is the meaning of "Vector $\vec e$ has $(n-2) \equiv (3-2) \equiv 1$ degree of freedom"?

### Solution

  We have shown that $\sum_i e_i = 0$ and $\sum x_i e_i = 0$. 
  
  Thus,
  
  $$3 + e_2 + e_3 = 0,$$
  
  and
  
  $$3 + 5e_2 + 3e_3 = 0.$$
  
  This provides us with a system of 2 equations and 2 unknowns.
  
  The linear system
  
|---|---|---|  
| 1  |  1 | -3  |
| 5  |  3 | -3  | 

# Problem #3

Given the fact that 

$$\frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)} \sim t_{n-2}$$

proceed to derive the formula of a general $(1- \alpha)%$ confidence interval for $\hat{\beta}_1$.

## Solution to Problem #3

Since 

$$\frac{\hat{\beta}_1 - \beta_1}{SE(\hat{\beta}_1)} \sim t_{n-2},$$

Recall that $\beta_1$ is the slope under the null hypothesis, $H_0:= \beta_1 = 0$. If we suppose the null hypothesis is true, then we have that the t-statistic under our assumption is 

$$t_{\hat{\beta}_1} = \frac{\hat{\beta}_1}{SE(\hat{\beta}_1)}.$$

Since $\hat{\beta}_1)$ is an unbiased estimator of $\beta_1$, we must have that $E(\hat{\beta}_1)) = \beta_1$. But under the null hypothesis, $\beta_1 = 0$, so the mean of our distribution is 0, and under the assumptions of the null hypothesis, the sampling distribution is normal. 

We also have that $$Var[\hat{\beta}_1)] = \frac{\sigma^2}{\sum (x_i - \bar{x})^2},$$

and since $SE(\hat{\beta}_1)) = \frac{\sigma}{\sqrt{}\sum (x_i - \bar{x})^2}$, we conclude that 

$$Var[\hat{\beta}_1)] \approx SE(\hat{\beta}_1))^2.$$

under the null hypothesis.


Finally we can say that $\hat{\beta}_1) \sim N(\beta_1, Var[\hat{\beta}_1])$ or that,

$$\hat{\beta}_1) \sim N(0, Var[\hat{\beta}_1])^2)$$,

leading to the confidence interval 

$\frac{\hat{\beta}_1}{SE(\hat{\beta}_1)} \pm t_{(1-\alpha/2, n-2)}$

implying
$$\hat{\beta}_1 \pm {SE(\hat{\beta}_1)}\cdot t_{(1-\alpha/2, n-2)}$$

as we wished to show.






# Problem #4

Provide the code that will conduct a 1000 replicates of the following:

  * Generate a sample $(x_1,y_1), (x_2, y_2), ..., (x_n, y_n)$ of size $n = 200$ from $Y = 2+3X + \epsilon, \ \ \epsilon \sim N(0,4^2)$
  * Calculate 90% confidence intervals resulting from $\hat{\beta}_0$ and $\hat{\beta}_1$ (just use _confint()_), __keep track of them.__.

and afterwards calculate the % of times (out of 1000 generated confidence intervals) that the true population values $\beta_0 = 2$ and $\beta_1 = 3$ ended up within their respective confidence intervals. Are those %'es equal to what we expected? Why? __Hint__: Recall the practical interpretation of a 90% confidence interval.

Code to get you started:

```{r}
set.seed(2)

n.rep <- 1000
conf_int_b0 <- matrix(0, nrow = n.rep, ncol = 2)
conf_int_b1 <- matrix(0, nrow = n.rep, ncol = 2)

for (r in 1:n.rep){
  X <- rnorm(200, mean = 0, sd = 1)
  eps <- rnorm(200, mean = 0, sd = 4)
  Y <- 2 + 3*X + eps
  # .... That's where you fit the linear model,
  lm.obj <- lm(Y~X)
  # calculate conf intervals for b0, b1, and save them into conf_int objects
  confints <- confint(lm.obj, level = 0.90)
  conf_int_b0[r,] <- confints[1,]
  conf_int_b1[r,] <- confints[2,]
}

# Here is where you calculated the %'es.
b0.filter <- conf_int_b0[,1] < 2 & 2 < conf_int_b0[,2]
filtered.conf_int_b0 <- conf_int_b0[b0.filter, ]
b0.succeses <- length(filtered.conf_int_b0[,1])
b0.perc <- b0.succeses/length(conf_int_b0[,1]) *100

b1.filter <- conf_int_b1[,1] < 3 & 3 < conf_int_b1[,2]
filtered.conft_int_b1 <- conf_int_b0[b1.filter,]
b1.succeses <- length(filtered.conft_int_b1[,1])
b1.perc <- b1.succeses/length(conf_int_b1[,1]) *100

print(c("The percentage of beta0 values that fell into the confidence interval is :", b0.perc, ", and the percentage of beta1 values that fell into the confidence interval is: ", b1.perc))
```

### Solution to Problem #4

The practical interpretation of the 90% confidence interval is that 90% of the time, the true mean will be between the two values. We can say that we are 90% confident that the true $\beta_0$ and $\beta_1$ values will lie in the confidence interval. The percentages we arrived at are not equal to what we expected, but that is because we did not perform enough trials. As the number of trials approaches infinity, the percent of confidence intervals which contain the true values will approach 90. This was easily checked by increasing _n.rep_ to 10000 trials.

# Problem #5

For _Advertisement.csv_ data set, proceed to fit simple linear regressions of

  * _sales_ onto _radio_
  * _sales_ onto _newspaper_.
  
For each regression

  1. Is there a _statistically_ significant relationship between predictor and response? Why?
  2. Is there a _practically_ significant relationship between predictor and response? Why?
  3. Interpret the confidence intervals for both the intercept and the slope.
  4. Provide prediction for a $20,000 investment into this advertisement media. Provide the confidence and prediction bands for it. Which ones are wider? Why?
  5. Report and interpret the Residual Standard Error (RSE).
  6. Report and interpret the $R^2$ statistic.
  
```{r}
sales.radio <- advert.df %>% select(sales, radio)
sales.newspaper <- advert.df %>% select(sales, newspaper)
```

```{r}
lm.radio <- lm(sales ~ radio, sales.radio)

summary(lm.radio)

```
1. We can conclude that there is a statistically significant relationship between _radio_ and _sales_, since the $p$-value for the regression coefficient is $< 2e^{-16}$.
2. However, this is not a practically significant relationship, since the 
  
```{r}
lm.newspaper <- lm(sales ~ newspaper, sales.newspaper)

summary(lm.newspaper)

```
1. We can conclude that since the $p$-value for the regression coefficient for _newspaper_ is not sufficiently low, the relationship between _newspaper_ and _sales_ is not statistically significant.
2. 
# Problem 6

Presume we're trying to explain a person's weight ($y$) via their height ($x_1$) and exercise hours per day ($x_2$).

  1. Write down the appropriate full modeling equation for multiple linear regression (including the assumptions on error terms.)
  
$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon, \ \ \epsilon \sim N(0,\sigma^2).$$



  2. __Derive__ the equations that will eventually lead to formulas for least squares estimates of all $\beta$-coefficients in your modeling equation from part (1) (those formulae can be found on slide #5 of "Multiple Linear Regression"). Proceed to __solve__ only the __first one__ (with respect to $\beta_0$)
  
### Derivation

We note that the goal is to find coefficients $\hat{\beta}_i$, $i = 0,1,2$ such that the RSS for the equation

$$\hat{y}_i = \hat{\beta}_{0} + \hat{\beta}_{1} x_{1,i} + \hat{\beta}_2 x_{2,i}$$

is minimized. That is, we must minimize,


$$\sum_i^n (\hat{y}_i - y_i)^2$$

$$= \sum_i^n (\hat{\beta}_{0} + \hat{\beta}_{1} x_{1,i} + \hat{\beta}_2 x_{2,i} - y_i)^2$$

We do so by solving for values which cause the vector of partial derivatives to be equal to $\vec 0$

__$\hat{\beta}_{0}$__

$$\frac{\partial RSS}{\partial \hat{\beta}_{0}} = 2\sum_i^n (\hat{\beta}_{0} + \hat{\beta}_{1} x_{1,i} + \hat{\beta}_2 x_{2,i} - y_i)$$

$$ = 2 \bigg(n\hat{\beta}_{0} + \hat{\beta}_{1}(x_{1,1} + ...+ x_{1,n} ) + \hat{\beta}_{2}(x_{2,1} + ...+ x_{2,n} ) - (y_1 + ... + y_n) \bigg)$$

$$ = \frac{2}{n} \bigg(\hat{\beta}_{0} + \hat{\beta}_{1}\bar{x}_1 + \hat{\beta}_{2}\bar{x}_2 - \bar{y} \bigg).$$

Setting equal to 0 and performing the basic algebra, we arrive at

$$\hat{\beta}_{0} = \bar{y} -\hat{\beta}_{1}\bar{x}_1 - \hat{\beta}_{2}\bar{x}_2$$

__$\hat{\beta}_{1}$__ (I can't figure out how the hell to do this one. Maybe I don't understand your question. Skipping it sorry!)

$$\frac{\partial RSS}{\partial \hat{\beta}_{1}} = 2\sum_i^n (\hat{\beta}_{0} + \hat{\beta}_{1} x_{1,i} + \hat{\beta}_2 x_{2,i} - y_i) x_{1,i}$$

$$2 \bigg(\hat{\beta}_{0}(x_{1,1} + ...+ x_{1,n} ) + \hat{\beta}_{1}(x_{1,1}^2 + ...+ x_{1,n}^2 ) + \hat{\beta}_{2}(x_{1,1}x_{2,1} + ...+ x_{1,n}x_{2,n} ) - (y_1 + ... + y_n) \bigg)$$
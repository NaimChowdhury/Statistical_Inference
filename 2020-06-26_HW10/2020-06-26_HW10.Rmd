---
title: 'Statistics Assignment #10'
author: "Naeem Chowdhury"
date: "6/26/2020"
output: pdf_document
---


##1. Setup
### options
Set up global options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
knitr::opts_chunk$set(fig.height=4, fig.width=6)
```

### libraries
Load in needed libraries 
```{r, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
#library(haven)
#library(plotly)
#install.packages('rgl')
#library(rgl)
library(ISLR)
```

## 2. File management
### Create variables for directories
```{r file_management, include=FALSE}
project.dir <- getwd() #naeem
output.dir <- "/Output"
data.dir <- "C:/Users/Naeem Cho/Desktop/School Work/Statistical Inference/Datasets"
setwd(project.dir)
getwd()
```

## 3. Importing Data
```{r, echo = FALSE, include=FALSE}
advertising <- read_csv(file.path(data.dir, "Advertising.csv"))
```


# Problem #1

We will work on the familiar _Advertising_ data set.

  1. First, let's try using just the _TV_ budget to predict the _Sales_.
    a. Fit the $Sales \sim TV$ regression model, provide the residuals-vs-fitted plot. Is there an issue? If yes, how do we fix it? Proceed to modify the model in order to fix it.
    b. Having fixed the issue from part 1(a), check the residuals-vs-fitted plot for modified model. Are there any other issues with the plot now? If yes, proceed to modify the model in order to fix it (__Hint__: polynomial regression).
    c. For your finalized model from part 1(b), provide the QQ-norm plot. Does the fit look normal? If not quite, is it that big of a deal for inference purposes? Why?
    d. For your finalized model from part 1(b), provide the plot of the data along with fitted line/curve (make sure to __enumerate__ the points using _text()_ function, like I did with _us_statewide_crime_ in Slides 5.R) Indicate observations that appear to be: 1) regression outliers; 2) high-leverage; 3) influential outliers.
    e. From part 1(d), pick the observation you believe to be the most influential outlier. Proceed to fit the regression 1) __with__ and 2) __without__ that observation included. Report the __fitted equations, RSE, and__ $R^2$ values, __comment on differences observed. DON'T INCLUDE THE OUTPUT OF $R$ COMMANDS__ (eval = F).
    f. What do you think we should do with that observation? Why?
    
## Solution 1.

### a.

  We first fit the model and plot the residuals and fitted plot.

```{r}
## Fitting model
lm.obj <- lm(sales ~ TV, data = advertising)

## Residuals vs. Fitted Plot
plot(lm.obj, which = 1)
```

  It is clear that the variance is increasing as the number of fitted values increases. We can attempt to solve this issue by modeling using the log of the response variable. This reduces the change in variance substantially.
  
```{r}
lm.obj <- lm(log(sales) ~ TV, data = advertising)
plot(lm.obj, which = 1)
```


### b.

Since the residual curve is not a line, but curved, there is evidence of non-linearity in the relationship. 

```{r}
plot1 <- advertising %>% 
  ggplot(aes(x = TV, y = log(sales))) +
  geom_point()+
  labs(title = "Nonlinearity of Sales vs TV")

plot1
```

The curvature seems to imply a polynomial relationship between the predictor and response, which we can capture by adding a polynomial term to the model.

Since the residuals vs fitted plot is pretty much linear, seem to have captured the non-linearity of the relationship with our model.
```{r}
## Fitting polynomial regression
lm.obj <- lm(log(sales) ~ TV + I(TV^2), data = advertising)

## Residuals vs Fitted of polynomial regression
plot(lm.obj, which = 1)
```


### c.

We can check for normality of fit by plotting the QQ-Norm. If the fit is roughly normal, we should expect the graph to have a roughly linear shape. It seems that our model deviates substantially from the line.

```{r}
## Produces QQ Norm Plot
plot(lm.obj, which = 2)
```


### d.

```{r}
plot2 <- advertising %>% 
  ggplot(aes(x = TV, y = log(sales), label = X1)) +
  geom_point()+
  geom_text(aes(label = as.character(X1)))

  labs(title = "Nonlinearity of Sales vs TV")

plot2
```

  From the plot we can see that points 131 and 156 are outliers. Since both are also in a "high-leverage range",  they are influential outliers.

### e.
. From part 1(d), pick the observation you believe to be the most influential outlier. Proceed to fit the regression 1) __with__ and 2) __without__ that observation included. Report the __fitted equations, RSE, and__ $R^2$ values, __comment on differences observed. DON'T INCLUDE THE OUTPUT OF $R$ COMMANDS__ (eval = F).

  I believe point 131 is the most influential outlier, since its low value $log(sales)$ value greatly influences the position of the intercept.
  
  1) Already fit with the point included.
  2) Fitting without point 131:
  
```{r, eval = F}
lm.obj.sub <- lm(log(sales) ~ TV + I(TV^2), data = advertising, subset = -131)

outlier <- summary(lm.obj)
no.outlier <- summary(lm.obj.sub)
```

Fitted equation with outlier:

$$ \widehat{sales} = 1.80 + 0.008\cdot TV - 0.000015 \cdot TV^2$$

Fitted equation __without__ outlier:

$$ \widehat{sales} = 1.86 + 0.007\cdot TV - 0.000013 \cdot TV^2$$

Comparing $R^2$:
```{r}
outlier$r.squared
no.outlier$r.squared
```

Comparing RSE:
```{r}
outlier$sigma
no.outlier$sigma
```

### f.

  Everything about these models is practically the same, from the coefficients to the $R^2$ and RSE. I wouldn't even remove the outlier.
    
  2. Now, let's try using all advertisement media to predict the $Sales$.
    a. Fit the $Sales \sim TV + radio + newspaper$ regression model. Prior to any residual diagnostics, first conduct __variable selection__. Which predictor(s) was dropped (if any)?
    b. For the model resulting from part 2(a), provide the residuals-vs-fitted plot. Is there an issue? If yes, how do we fix it? Proceed to modify the model in order to fix it. Rinse and repeat until you get a model that appears appropriate.
    c. For your finalized model from part 2(b), while still possible, but visualization of the fitted hyper-plane gets tougher. If we can't visualize the fit, what should we utilize in order to identify 1) regression outliers; 2) high-leverage observations; 3) influential outliers? Proceed to do it.
    d. From part 2(c), pick the observation that shows up as the most  influential outlier. Proceed to fit the rgression 1) __with__ and 2) __without__ that observation included. Report the __fitted equations, RSE, and__ $R^2$ values, __comment on differences observed. DON'T INCLUDE THE OUTPUT OF $R$ COMMANDS.
    e. What do you think we should do with that observation? Why?

## Solution 2.

### a.

```{r}
## Fitting the linear model
lm.obj <- lm(sales ~ TV + radio + newspaper, data = advertising)

## Variable selection
lm.obj <- step(lm.obj)
```

Newspaper is dropped by the variable selection process, based off of its AIC value.

### b.

```{r}
# Residuals vs fitted for variable selection model
plot(lm.obj, which = 1)
```

For the same reasons as the previous problem (curvature in the mean residual curve), we infer non-linearity in the relationship. The simplest way to introduce non-linearity is with an interaction term.

```{r}
## Fitting model with interaction
lm.obj <- lm(sales~ TV + radio + TV:radio, data = advertising)

# Residuals vs fitted
plot(lm.obj, which = 1)
```

Unfortunately this plot also has significant curvature. It may be that one of the predictors has a quadratic relationship with the response, so we can check for that relationship with plots.

```{r}
plot1 <- advertising %>% 
  ggplot(aes(x = TV, y =sales)) +
  geom_point() + 
  labs(title = "Sales vs TV")

plot1

plot2 <- advertising %>% 
  ggplot(aes(x = radio, y = sales)) +
  geom_point() + 
  labs(title = "Sales vs Radio")

plot2
```

  Curvature in the $sales \sim TV$ plot indicates a quadratic relationship. We can test this idea by using the residuals vs fitted plot to check the quadratic model with interaction.
  
```{r}
# Fitting model with interaction and quadratic terms
lm.obj <- lm(sales ~ TV + radio + TV:radio + I(TV^2), data = advertising)

# Residual vs Fitted
plot(lm.obj, which = 1)
```
  
The line has a (qualitatively) consistent variance and a straight average error line.

### c.

### d.

### e.

# Problem #2

  1. For _Carseats_ data set of _ISLR_ package, proceed to work on the following model:
  
  $$Sales \sim Income + Advertising + ShelveLoc + Urban$$
    a. Provide the full modeling equation (with assumptions on error terms).
    b. Proceed to check if hte model assumptions hold via diagnosing appropriate plots.
    c. Given your answer in part 1(b), can we rely on $p$-values provided by classic inference here? Proceed to interpret two most statistically significant effects.
    
## Solution 1.

### a.

### b.

### c.
  
  2. For _Boston_ data set of _ISLR_ package, proceed to work on the following model:
  
  $$medv \sim crim + rm + lstat$$
    a. Provide the full modeling equation (with assumptions on error terms).
    b. Proceed to check if the model assmptions hold via diagnosing appropriate plots. If they don't, proceed to adjust the model in some ways in order to improve the fit. _Hint:_ Plot $medv$ against each predictor and diagnose if there's potentially a non-linear relationship.
    c. For the terms you've added in part 2(b) to the initial model from part 2(a), proceed to test the hypotheses for their statistical significance.
    
## Solution 2.

### a.

### b.

### c.
    
# Problem #3 (+3 Bonus Points)

For _thuesen_ data set of _ISwR_ library, proceed to use bootstrap in order to calculate standard errors and 95% confidence interval for $\hat{\beta}_1$ in the following regression:

$$short.velocity_i = \beta_0 + \beta_1 \cdot blood.glucose_i + \epsilon_i, \ \ \epsilon_i \sim_{i.i.d} N(0, \sigma^2)$$

Compare your results with the classic inference (obtained via fitting the model for full data set, no boostrapping). Did your conclusion about significance of blood glucose predictor differ at $\alpha = 0.05$ for bootstrap vs. classical approach? If  yes, why do you think that is?
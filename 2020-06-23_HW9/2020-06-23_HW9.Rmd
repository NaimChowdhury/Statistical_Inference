---
title: 'Statistics Assignment #9'
author: "Naeem Chowdhury"
date: "6/16/2020"
output: pdf_document
---


##1. Setup
### options
Set up global options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
knitr::opts_chunk$set(fig.height=4, fig.width=6)
```

### libraries
Load in needed libraries 
```{r, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(ggplot2)
#library(haven)
#library(plotly)
#install.packages('rgl')
#library(rgl)
library(ISLR)
```

## 2. File management
### Create variables for directories
```{r file_management, include=FALSE}
project.dir <- getwd() #naeem
output.dir <- "/Output"
data.dir <- "C:/Users/Naeem Cho/Desktop/School Work/Statistical Inference/Datasets"
setwd(project.dir)
getwd()
```

## 3. Importing Data
```{r, echo = FALSE, include=FALSE}
advertising <- read_csv(file.path(data.dir, "Advertising.csv"))

CPU.df <- read_csv(file.path(data.dir, "Intel_CPUs.csv"))
```

# Problem #1

  1. See slides #54-55. __Hint:__ At one point in your derivation, make sure to utilize the fact that $D_{fem} = 1$ for females, 0 for males. It will allow you to "break the sum": $\sum_{i = 1}^n = \sum_{i \in Females} + \sum_{i \in Males}$.
  
## Solution 1.1

  We first note that the full regression equation is:
  
  $$y_i = \beta_0 + \beta_1 D_{Fem,i} + \epsilon_i$$
  
Then the RSS is

$$\sum_i^n (y_i - \hat{y}_i)$$

$$ = \sum_i^n \bigg(y_i - (\hat{\beta_0} + \hat{\beta_1} D_{Fem}) \bigg)$$

Solving for $\hat{\beta_k}$ that minimizes the equation, we have to solve for where the vector of partial derivatives is equal to 0.

$$\frac{\partial RSS}{\partial \hat{\beta_0}} = 2\sum_i^n (y_i - \hat{\beta_0} - \hat{\beta_1} D_{Fem})(-1)$$



$$= -2\sum_i^n (y_i - \hat{\beta_0} - \hat{\beta_1} D_{Fem})$$

$$= -2\sum_i^n y_i - \sum_i^n\hat{\beta_0} - \sum_{i \in Females} \hat{\beta_1} D_{Fem} + \sum_{i \in Males} \hat{\beta_1} D_{Fem}$$


Since $D_{Fem}) = 0$ when $i \in Males$, this reduces to

$$= -2(n\cdot \bar{y} - n\cdot \hat{\beta_0} - k \cdot \hat{\beta_1})$$

where $k$ is the number of females in the sample. 

In order to minimize RSS, we must have 

$$0 = n\cdot \bar{y} - n\cdot \hat{\beta_0} - k \cdot \hat{\beta_1}$$

Implying that

$$\hat{\beta_0} = \bar{y} -\hat{\beta_1}(k/n).$$

For $\hat{\beta_1}$, similarly:

$$\frac{\partial RSS}{\partial \hat{\beta_1}} = 2\sum_i^n (y_i - \hat{\beta_0} - \hat{\beta_1} D_{Fem})(-D_{Fem})$$

$$= -2\sum_i^n (y_i D_{Fem} - \hat{\beta_0} D_{Fem} - \hat{\beta_1} D^2_{Fem})$$

$$= -2\sum_i^n (y_i D_{Fem} - \hat{\beta_0} D_{Fem} - \hat{\beta_1} D^2_{Fem}).$$

Since $\sum_{i \in Males} D_{Fem} = 0$, this reduces to

$$= -2 \bigg(\sum_{i \in Females}y_i - \sum_{i \in Females} \hat{\beta_0} - \sum_{i \in Females} \hat{\beta_1} \bigg).$$

Let $k$ be the number of females in the data set, and $\bar{y}_k$ be the average of balances for females. Then


$$ = -2(k \cdot \bar{y}_k - k \cdot \hat{\beta_0} - k \cdot \hat{\beta_1})$$

Which, by our previous computation

$$ = -2(k \cdot \bar{y}_k - k \cdot (\bar{y} -\hat{\beta_1}(k/n)) - k \cdot \hat{\beta_1})$$ 

$$ = -2(k \cdot \bar{y}_k - k \cdot \bar{y} -k\cdot \hat{\beta_1}( (k/n) -1)$$ 

Let $m = n - k$ be the number of males in the data set. Then

$$ = -2k((\bar{y}_m) + \hat{\beta_1}(m/n))$$ 

Implying,

$$\hat{\beta_1} = -\bar{y}_m \cdot \frac{n}{m}$$


  2. On slide #60, attempt to interpret the effect of what happens "per 1-unit increase in $D_{Asia}$, while holding other variables constant". Why is it not possible to evaluate that effect here? Hence, to avoid that, how many dummy variables should we use in order to model a categorical predictor in this case? Show that now, having dropped $D_{Afr-Am}$, we can actually evaluate the aforementioned effect of $D_{Asia}$.
  
## Solution 1.2

  $D_{Asia}$ is a binary variable that cannot increase by one unit, but instead be "on" or "off". When $D_{Asia}$ is "on", the other variables $D_{Cauc}$ and $D_{Afr-Am}$ must be "off". However, the coefficients $\hat{\beta}_j$ are not uniquely defined for the problem, 
  
  3. When dealing with a categorical predictor $X$ containing > 2 categories (say, 3), why do we have to use dummy variables approach? Why not just model it as
  
  $$Piecewise function here$$.

# Problem #2

For the _Wage_ data set from _ISLR_ package, we will use _age_ and _race_ to predict person's _wage_ (in $1000, I believe).

  1.
    a. Proceed to write down the full modeling equation for $wage \sim age + race$ regression.
    b. Fit the model from 1(a), and write down the fitted equation.
    c. Interpret the most statistically significant __dummy variable__ (NOT the "per 1-unit" version though, the group comparison version).
    d. Conduct the test for significance of _race_ when predicting person's wage. In particular, make sure to 1) formulate the $H_0$, $H_a$ hypotheses; 2) Write down the modeling equation of the "null" model; 3) write the formula for test statistic; 4) use _R_'s _anova()_ to carry out the test, and match the output to the terms in the test statistic formula; 5) Provide the conclusion of the test.

## Part 1.

### 1a.

```{r}
levels(Wage$race) 
```

Since race has 4 levels,

$$wage_i = \beta_0 + \beta_1 \cdot age_i + \beta_2 \cdot D_{Black, i} + \beta_3 \cdot D_{Asian, i} + \beta_4 \cdot D_{Other,i} + \epsilon_i, \ \ \epsilon \sim N(0, \sigma^2),$$

### 1b.

```{r}
lm.obj <- lm(wage ~ age + race , Wage)
summary(lm.obj)
```

Thus the fitted equation is,

$$\hat{wage}_i = 82.42 + 0.71 \cdot age_i - 11.79 \cdot D_{Black, i} + 8.13 \cdot D_{Asian, i} -19.25 \cdot D_{Other,i}$$ 

### 1c.

The most statistically significant dummy variable is $D_{Black}$ with $p < 2 \times 10^{-6}$. 

When _age_ is fixed, black employees make about \$11,800 dollars less than white employees, on average.

### 1d. 

To conduct the test for significance of the model, we use the $F$-test, with hypotheses:

$$H_0:= (\beta_i = 0, \ \ \forall i \in \{2,3,4\})$$

$$H_a:= (\exists i \in \{2,3,4\} \ \ st. \ \ \beta_i \neq 0)$$.


Under the null hypothesis, the equation for the model becomes

$$wage_i = \beta_0 + \beta_1 \cdot age_i + \epsilon_i, \ \ \epsilon_i \sim_{i.i.d} N(0, \sigma^2)$$

Finally, we conduct the test

```{r}
lm.full.obj <- lm(wage ~ age + race, data = Wage)
lm.null.obj <- lm(wage ~ age, data = Wage)

anova(lm.null.obj, lm.full.obj)

```

Recalling that the $F$-statistic is defined as

$$FS = \frac{(Reg_{full} - Reg_{null})/q}{RSS_{full}/(n - (p+1))}$$

$$ = \frac{(5022216 - 4957481)/3}{4957481/2995}$$

```{r}
((5022216 - 4957481)/3)/(4957481/2995) 
```

, which agrees precisely with the $F$ statistic presented in the _anova()_. We conclude by rejecting the null hypothesis $H_0$, since $p \approx 1.86 \times 10^{-8} < 0.05$, concluding $race$ is significant in the model.

  2. 
    a. Provide data visualization (use your fancy _ggplot2_) that will help determining if there's a strong interaction between _race_ and _age_ when predicting _wage_. Comment on whether you observe an interaction. Explain. __NOTE(!)__: You shouldn't start fitting any models with interactions yet.
    b. Proceed to confirm your hunch from part 2(a) by actually conducting an appropriate statistical test for significance of interaction. In particular, make sure to 1) write the full modeling equation __with interaction__; 2) formulate the $H_0$, $H_a$ hypotheses; 3) write down the modeling equation of the "null" model; 4) use _R_'s _anova()_ to carry out the test (no need to explain details here); 5) Provide the conclusion of the test.
    
## Part 2.

### 2a.

We can start by first graphing the relationship between age and wage, controlling for race.

```{r}
ggplot(Wage, aes(x = age, y = wage, col = race)) +
  geom_smooth(method = 'lm') +
  xlab("Age")+
  ylab("Wage (in thousands)")+
  labs(title = "Wage as predicted by age, controlling for race")
```

Since two of the lines intersect, there is some evidence of potential interaction between predictors, but it is not clear.

### 2b.

  1. To test for the possibility of interaction, we consider the full modeling equation

$$wage_i = \beta_0 + \beta_1 \cdot age_i + \beta_2 \cdot D_{Black, i} + \beta_3 \cdot D_{Asian, i} + \beta_4 \cdot D_{Other,i} + \beta_5 (age \cdot D_{Black,i}) + \beta_6 (age \cdot D_{Asian,i}) + \beta_7 (age \cdot D_{Other,i}) + \epsilon_i, \ \ \epsilon \sim N(0, \sigma^2).$$

  2. To test for interaction we work under the following hypotheses:

$$H_0:= (\beta_i = 0, \ \ \forall i \in \{5,6,7\})$$

$$H_a:= (\exists i \in \{2,3,4\}\ \ st. \ \ \beta_i \neq 0)$$.

  3. The "null" model would be 

$$wage_i = \beta_0 + \beta_1 \cdot age_i + \beta_2 \cdot D_{Black, i} + \beta_3 \cdot D_{Asian, i} + \beta_4 \cdot D_{Other,i} + \epsilon_i, \ \ \epsilon \sim N(0, \sigma^2).$$

  4. 
  
```{r}
lm.full.obj <- lm(wage ~ age + race + age:race, data = Wage)
lm.null.obj <- lm(wage ~ age + race, daga = Wage)

anova(lm.null.obj, lm.full.obj)
```

Since $p = 0.1998 > 0.05$, under a confidence level of 90% we fail to reject the $H_0$ hypothesis. We can conclude that there is no statistically significant interaction between _age_ and _race_.

    
# Problem #3

Find a data example on your own (not among the ones already used in class) where you have a categorical predictor $D$, quantitative predictor $x$, and a quantitative response $y$, such that there's actually a __significant interaction between__ $x$ __and__ $D$ __in explaining__ $y$. Then,

  1. Make sure to demonstrate that interaction graphically (prior to fitting the interaction model).
  
## Part 1.

  2. Write down 1) the full modeling equation with interaction terms, and 2) separate modeling equations for different categories of $D$. Comment on differences between separate modeling equation. Fill the blank in the sentence: "Interaction terms allow for difference in ... across the categories."
  
## Part 2.

  3. Confirm statistical significance of interaction by conducting an appropriate test (just conduct it in $R$ and provide the conclusion, no need to as much detail as problem \#2).
  
## Part 3.
  
  4. Interpret the interaction, as well, by both
  
    a. Writing out a separate fitted equation for each category of $D$,
    b. Providing effect displays,
    
  and commenting on those.
  
## Part 4.

  5. Try interpreting the __main effects__ of $D_1$ (for juse one of the dummy variables) and $x$. Does the interpretation sound "all-encompassing"? Hence, should we ever rely on interpretations/significance tests of main effects in models with strong interactions?
  
## Part 5.
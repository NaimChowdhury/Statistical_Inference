---
title: "Stats. Inference Assignment 4"
author: "Naeem Chowdhury"
date: "5/25/2020"
output: pdf_document
---


##1. Setup
### options
Set up global options
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(tidy=TRUE)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=70))
knitr::opts_chunk$set(fig.height=4, fig.width=6)
```

### libraries
Load in needed libraries 
```{r, include=FALSE}
library(tidyverse)
library(RColorBrewer)
library(haven)
```

## 2. File management
### Create variables for directories
```{r file_management, include=FALSE}
project.dir <- getwd() #naeem
output.dir <- "/Output"
data.dir <- "C:/Users/Naeem Cho/Desktop/School Work/Statistical Inference/Datasets"
setwd(project.dir)
getwd()
```

## 3. Importing Data
```{r}
cpu.data <- read_csv(file.path(data.dir, "Intel_CPUs.csv"))
```


**The goal of this homework is to have you 1) Familiarized with the $\chi^2$ test of independence for contingency tables; 2) measuring practical strength of dependence between categorical variables; 3) familiarized with permutation test for contingency tables; 4) practice your R coding.**

# Problem #1

## 1. For all three examples on slide #24, proceed to

```{r}
case.A <- matrix(c(51, 49, 49, 51), ncol = 2, byrow = TRUE)
colnames(case.A) <- c("Yes", "No")
rownames(case.A) <- c("Female", "Male")
case.A <- as.table(case.A)

case.B <- matrix(c(102, 98, 98, 102), ncol = 2, byrow = TRUE)
colnames(case.B) <- c("Yes", "No")
rownames(case.B) <- c("Female", "Male")
case.B <- as.table(case.B)

case.C <- matrix(c(5100, 4900, 4900, 5100), ncol = 2, byrow = TRUE)
colnames(case.C) <- c("Yes", "No")
rownames(case.C) <- c("Female", "Male")
case.C <- as.table(case.C)
```


### a. Use the _my.chisq.test()_ function you've defined in the previous HW in order to confirm the $X^2$ and $p$-values. __Hint:__ Make sure to convert the %'s into counts first.

Establishing _my.chisq.test()_:
```{r}
my.chisq.test <- function(ctable) {
  rows <- nrow(ctable)
  cols <- ncol(ctable)
  
  rowsums <- rowSums(ctable)
  colsums <-colSums(ctable)
  
  total <- sum(ctable)
  df <- (rows-1)*(cols-1)
  
  expected <- matrix(0, nrow = rows, ncol = cols)
  
  for(i in 1:rows){
    
    for(j in 1:cols){
      
      expected[i,j] <- (rowsums[i]*colsums[j])/ total
      
    }
  }
  chi_sq <- ((ctable - expected)^2)/expected
  chi_sq <- sum(chi_sq)
  
  p <- pchisq(chi_sq, df, lower.tail = FALSE)
  
  
  print(c("X-Squared: ",  chi_sq))
  print(c("Degrees of Freedom: ", df))
  print(c("p-value: ", p))
  # print(c("Matrix of expected counts: ", expected))
}
```

```{r}
#Case A
my.chisq.test(case.A)
```

```{r}
#Case B
my.chisq.test(case.B)
```
```{r}
#Case C
my.chisq.test(case.C)
```

The results of all 3 tests line-up with the results of the tests presented in the slides!

### b. Calculate the difference in proportion between males and females that attend religious services weekly. Calculate the risk ratio between males and females that attend religious services weekly.

The data in the original tables is already set up as a proportion.

```{r}
#Convert case A table into a table of conditional proportions
props <- case.A*0.01
#Compute the difference of proportions
diffprop <- props[1]-props[3]
print(c("The difference of proportions is ", diffprop))

riskratio <- props[1]/props[3]
riskratio
```

### c. Based on your answers to parts (a) - (b), as $n$ increases, what do you notice with respect to statistical significance? Practical significance?

As $n$ increases, the statistical signifance of the $\chi^2$-test increases. However, the practical significance remains precisely the same for each experiment, showing very little practical significance.


## 2. Do exercises **11.32** and **11.33** from the **Agresti** book.

## 11.32 Marital Happiness

The table shows 2012 GSS data on marital and general happiness for married respondents.

```{r}
marital.happy <- matrix(c(11, 11, 4, 31, 215, 34, 20, 231, 337), ncol = 3, byrow = TRUE)
colnames(marital.happy) <- c("Not Too Happy", "Happy", "Very Happy")
rownames(marital.happy) <- c("Not Too Happy", "Happy", "Very Happy")
marital.happy <- as.table(marital.happy)

marital.happy
```

### a. The chi-squared test of independence has $X^2 = 214$. What conclusion would you make using a significance level of 0.05? Interpret.

```{r}
df <- (3-1)*(3-1)
pchisq(214, df, lower.tail = FALSE)
```
The value $X^2 = 214$ is very deep into the tail of the $\chi^2$ distribution under the null hypothesis of independence with $df = 4$. In fact, this results in an extremely low $p$-value, and we can conclude by rejecting the null hypothesis. In context, it means that either Marital Happiness is dependent on General Happiness, or General Happiness is dependent on Marital Happiness.

### b. Does this large chi-squared value imply there is a strong association between marital and general happiness? Explain.

The large chi-squared value alone does not imply any strength of association, only a dependence relation between the variables. Instead we look to strength of relationship metrics.

### c. Find the difference in the proportion of being not too happy between those that are not too happy in their marriage and those that are very happy in their marriage. Interpret that difference.

```{r}
marginal1 <- sum(marital.happy[1,])
marginal3 <- sum(marital.happy[3,])

prop1 <- marital.happy[1,1]/marginal1
prop2 <- marital.happy[3,1]/marginal3

diffprop <- prop1 - prop2
diffpoints <- diffprop*100

print(c("The proportion of people who are generally 'Not Too Happy' is ", diffpoints, "percentage points higher for those who are 'Not Too Happy' in their marriage than 'Very Happy' in their marriage."))
```

### d. Find and interpret the relative risk of being not too happy, comparing the lowest and highest marital happiness group. Interpet.

```{r}
relativerisk <- prop1/prop2

relativerisk

print(c("People who are 'Not Too Happy' in their marriage are ", relativerisk, "times more likely to be 'Not Too Happy' in general than those who are 'Very Happy' with their marriage."))
```


# Problem #2

## 1. Code up your own _my.permutation.test()_ function to conduct permutation tests on contingency tables. As inputs, it should take a dataframe with two categorical variables as columns (first one - explanatory, second one - response), and the number of randomly generated permutationts to be executed. As output, it should provide:

    - contingency table for the data frame,
    
    - permutation $p$-value,
    
    - plot the histogram of permutation distribution for $X^2$ statistic.
    
  To obtain $X^2$ values for each permutation, you are allowed to use $R$'s _chisq.test()_ function.
  
## 2. Proceed to apply the _my.permutation.test()_ function (and subsequently interpret the results) to:

```{r}
my.permutation.test <- function(data, predictor){
  # Get the length of the data
  nrows <- nrow(data)
  seq <- seq(1, nrows)
  chis <- matrix(NA, nrow = 10000)
  for(i in 1:10000){
    perm.sample <- sample.int(nrows, size = nrows, replace = FALSE)
    perm.data <- data[perm.sample, ]
    datatable.perm <- table(perm.data)
    chis[i] <- chisq.test(datatable.perm)[1]
  }
  return(chis)
}



```


### a. GPU_data, with 10,000 permutations. What's the conclusion? Compare the resulting histogram with the one in the slides (they should be roughly similar).

### b. Intel_CPU data, with just 1,000 permutations. What's the conclusion? Compare the shape of the resulting histogram with the density of $\chi^2$ distribution with appropriate degrees of freedom. What does it tell us about whether $\chi^2$-test results from HW3 were appropriate for Intel_CPU data?


# Problem #3

## 1. Complete the **lab** by wrapping everything we did into a nice, concise, R markdown report (just the code, with necessary comments, contingency table, test results, and interpretations).

## 2. Moreover, finish the **lab** by applying your _my.permutation.test()_ to inferring independence of court surface and match outcomes for Nadal vs. Federer, Serena vs Venus match-ups. Formulate the hypotheses, explain why $\chi^2$-test might be inappropriate, and deliver the conclusion. Compare the conclusion with the one we got from $\chi^2$-test during the lab.